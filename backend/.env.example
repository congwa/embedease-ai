# ========================================
# LLM 提供商配置
# ========================================
# 支持的提供商: openai, anthropic, deepseek, siliconflow 等
# 所有兼容 OpenAI API 格式的提供商都可以使用

# 主要 LLM 提供商
LLM_PROVIDER=siliconflow
LLM_API_KEY=sk-your-api-key-here
LLM_BASE_URL=https://api.siliconflow.cn/v1
LLM_CHAT_MODEL=moonshotai/Kimi-K2-Thinking

# ========================================
# Embeddings 配置
# ========================================
# 可以使用与 LLM 不同的提供商（例如使用更便宜的服务）
# 如果不设置 EMBEDDING_API_KEY 和 EMBEDDING_BASE_URL，将使用 LLM 的配置

EMBEDDING_PROVIDER=siliconflow
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-8B
EMBEDDING_DIMENSION=4096

# 如果 Embeddings 使用不同的提供商，取消注释以下配置：
# EMBEDDING_API_KEY=sk-your-embedding-api-key
# EMBEDDING_BASE_URL=https://api.your-embedding-provider.com/v1

# ========================================
# Rerank 配置
# ========================================
# 重排序功能，用于提升检索结果的相关性

RERANK_ENABLED=true
RERANK_MODEL=Qwen/Qwen3-Reranker-8B
RERANK_TOP_N=5
RERANK_INSTRUCTION=根据查询对商品进行相关性排序

# 如果 Rerank 使用不同的提供商，取消注释以下配置：
# RERANK_PROVIDER=siliconflow
# RERANK_API_KEY=sk-your-rerank-api-key
# RERANK_BASE_URL=https://api.your-rerank-provider.com/v1

# ========================================
# 模型能力配置
# ========================================
# 手动指定模型能力（JSON 格式），会覆盖 models.dev 的配置
# 示例：
# MODEL_PROFILES_JSON={"moonshotai/Kimi-K2-Thinking": {"reasoning_output": true, "tool_calling": true, "structured_output": true}}
MODEL_PROFILES_JSON=

# models.dev 配置（自动获取模型能力信息）
MODELS_DEV_ENABLED=true
MODELS_DEV_API_URL=https://models.dev/api.json
# MODELS_DEV_PROVIDER_ID 默认使用 LLM_PROVIDER，也可以手动指定
# MODELS_DEV_PROVIDER_ID=siliconflow
MODELS_DEV_TIMEOUT_SECONDS=10.0
MODELS_DEV_CACHE_TTL_SECONDS=86400.0

# ========================================
# Qdrant 向量数据库配置
# ========================================
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=products

# ========================================
# 数据库配置
# ========================================
DATABASE_PATH=./data/app.db
CHECKPOINT_DB_PATH=./data/checkpoints.db

# ========================================
# 文本处理配置
# ========================================
CHUNK_SIZE=800
CHUNK_OVERLAP=100

# ========================================
# 服务配置
# ========================================
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=http://localhost:3000

# ========================================
# 日志配置
# ========================================
LOG_LEVEL=DEBUG
LOG_MODE=detailed
LOG_FILE=./logs/app.log
LOG_FILE_ROTATION=10 MB
LOG_FILE_RETENTION=7 days

# ========================================
# 聊天模式配置
# ========================================
# natural: 商品推荐助手模式（默认），专注商品推荐，非商品问题引导回商品
# free: 自由聊天模式，可聊任何话题，工具可用但不强制
# strict: 严格模式，必须基于工具输出或历史对话回答，否则返回受控失败
CHAT_MODE=natural

# ========================================
# Agent 工具执行配置
# ========================================
# 工具串行执行：当模型一次返回多个 tool_calls 时，是否强制按顺序执行（而非并行）
# true: 工具调用串行执行，避免并发问题，适合工具间有依赖关系的场景
# false: 工具调用并行执行（默认 LangGraph 行为），执行速度更快
AGENT_SERIALIZE_TOOLS=true

# ========================================
# Agent TODO 规划中间件配置
# ========================================
# 启用后，Agent 会自动注入 write_todos 工具和规划提示
# 用于复杂多步任务（如多商品/多类别/多价格区间）的规划与跟踪
# 模型会在遇到复杂任务时主动创建待办清单，逐步执行并标记完成

AGENT_TODO_ENABLED=true
# 自定义系统提示（可选，留空使用默认的 WRITE_TODOS_SYSTEM_PROMPT）
# 示例：在默认提示前添加业务语境
# AGENT_TODO_SYSTEM_PROMPT=当存在多商品、多类别或多价格区间时，先使用 write_todos 规划步骤。
AGENT_TODO_SYSTEM_PROMPT=
# 自定义工具描述（可选，留空使用默认的 WRITE_TODOS_TOOL_DESCRIPTION）
AGENT_TODO_TOOL_DESCRIPTION=

# ========================================
# Agent 工具调用限制中间件配置
# ========================================
# 限制工具调用次数，防止 Agent 陷入无限循环
# 当达到限制时，根据 EXIT_BEHAVIOR 决定行为

AGENT_TOOL_LIMIT_ENABLED=true
# 线程级限制（跨多次 run 累计），留空表示不限制
# AGENT_TOOL_LIMIT_THREAD=50
AGENT_TOOL_LIMIT_THREAD=100
# 单次 run 限制，默认 20 次
AGENT_TOOL_LIMIT_RUN=20
# 超限行为:
# - continue: 阻止超限的工具调用，但允许其他工具继续执行（默认）
# - error: 抛出异常，立即停止
# - end: 立即结束，注入 ToolMessage + AIMessage 告知用户
AGENT_TOOL_LIMIT_EXIT_BEHAVIOR=continue

# ========================================
# Agent 工具重试中间件配置
# ========================================
# 工具调用失败时自动重试，支持指数退避

AGENT_TOOL_RETRY_ENABLED=true
# 最大重试次数（不含首次调用）
AGENT_TOOL_RETRY_MAX_RETRIES=2
# 指数退避因子（每次重试延迟 = initial_delay * backoff_factor^attempt）
AGENT_TOOL_RETRY_BACKOFF_FACTOR=2.0
# 初始延迟（秒）
AGENT_TOOL_RETRY_INITIAL_DELAY=1.0
# 最大延迟（秒），超过此值则封顶
AGENT_TOOL_RETRY_MAX_DELAY=60.0

# ========================================
# Agent 上下文压缩中间件配置
# ========================================
# 当消息历史过长时自动压缩（使用 LLM 生成摘要替换旧消息）
# 压缩发生时会推送 context.summarized 事件到前端

AGENT_SUMMARIZATION_ENABLED=true
# 触发压缩的阈值（消息数），达到此数量时触发压缩
AGENT_SUMMARIZATION_TRIGGER_MESSAGES=50
# 压缩后保留的最近消息数
AGENT_SUMMARIZATION_KEEP_MESSAGES=20
# 用于生成摘要的最大 token 数（避免摘要请求过大）
AGENT_SUMMARIZATION_TRIM_TOKENS=4000

# ========================================
# 商品库画像配置
# ========================================
# 导入商品时自动生成画像摘要，注入 Agent system prompt 指导检索方向
CATALOG_PROFILE_ENABLED=true
CATALOG_PROFILE_TTL_SECONDS=600
CATALOG_PROFILE_TOP_CATEGORIES=3

# ========================================
# 多提供商配置示例
# ========================================

# 示例 1: 使用 OpenAI
# LLM_PROVIDER=openai
# LLM_API_KEY=sk-your-openai-key
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_CHAT_MODEL=gpt-4
# EMBEDDING_MODEL=text-embedding-3-large
# EMBEDDING_DIMENSION=3072

# 示例 2: 使用 DeepSeek
# LLM_PROVIDER=deepseek
# LLM_API_KEY=sk-your-deepseek-key
# LLM_BASE_URL=https://api.deepseek.com/v1
# LLM_CHAT_MODEL=deepseek-chat
# EMBEDDING_MODEL=deepseek-embedding
# EMBEDDING_DIMENSION=1536

# 示例 3: 使用 Anthropic
# LLM_PROVIDER=anthropic
# LLM_API_KEY=sk-your-anthropic-key
# LLM_BASE_URL=https://api.anthropic.com/v1
# LLM_CHAT_MODEL=claude-3-5-sonnet-20241022

# 示例 4: 混合使用（LLM 用 OpenAI，Embeddings 用 SiliconFlow）
# LLM_PROVIDER=openai
# LLM_API_KEY=sk-your-openai-key
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_CHAT_MODEL=gpt-4
# 
# EMBEDDING_PROVIDER=siliconflow
# EMBEDDING_API_KEY=sk-your-siliconflow-key
# EMBEDDING_BASE_URL=https://api.siliconflow.cn/v1
# EMBEDDING_MODEL=Qwen/Qwen3-Embedding-8B
# EMBEDDING_DIMENSION=4096

# ========================================
# 记忆系统配置
# ========================================

# === 总开关 ===
MEMORY_ENABLED=true

# === LangGraph Store（长期记忆基座）===
# 跨会话用户画像存储，存放用户偏好、任务进度等
MEMORY_STORE_ENABLED=true
MEMORY_STORE_DB_PATH=./data/memory_store.db

# === Memory 专用模型配置 ===
# 可以与聊天模型不同，例如使用更便宜/更快的模型进行记忆抽取
# 留空则使用主 LLM 配置
MEMORY_MODEL=
MEMORY_PROVIDER=
MEMORY_API_KEY=
MEMORY_BASE_URL=

# 示例：使用更便宜的模型进行记忆处理
# MEMORY_MODEL=deepseek-chat
# MEMORY_PROVIDER=deepseek
# MEMORY_API_KEY=sk-your-deepseek-key
# MEMORY_BASE_URL=https://api.deepseek.com/v1

# === 事实型长期记忆 ===
# LLM 事实抽取 → 哈希去重 → Qdrant 向量检索
MEMORY_FACT_ENABLED=true
MEMORY_FACT_DB_PATH=./data/facts.db
MEMORY_FACT_COLLECTION=memory_facts
MEMORY_FACT_SIMILARITY_THRESHOLD=0.5
MEMORY_FACT_MAX_RESULTS=10

# === 图谱记忆 ===
# 实体/关系/观察的结构化知识图谱
MEMORY_GRAPH_ENABLED=true
MEMORY_GRAPH_FILE_PATH=./data/knowledge_graph.jsonl

# === 记忆编排 ===
# 三阶段编排：检索判定、上下文注入、异步写入
MEMORY_ORCHESTRATION_ENABLED=true
MEMORY_ASYNC_WRITE=true

# ========================================
# 网站爬取模块配置
# ========================================
# 从网站自动发现商品并导入，支持 SPA 网站和 LLM 智能解析

# === 总开关 ===
CRAWLER_ENABLED=false

# === 爬取专用模型配置 ===
# 可以与聊天模型不同，例如使用更便宜/更快的模型进行页面解析
# 留空则使用主 LLM 配置
CRAWLER_MODEL=
CRAWLER_PROVIDER=
CRAWLER_API_KEY=
CRAWLER_BASE_URL=

# 示例：使用更便宜的模型进行页面解析
# CRAWLER_MODEL=deepseek-chat
# CRAWLER_PROVIDER=deepseek
# CRAWLER_API_KEY=sk-your-deepseek-key
# CRAWLER_BASE_URL=https://api.deepseek.com/v1

# === 浏览器配置 ===
# 使用 Playwright 进行 SPA 页面渲染
CRAWLER_HEADLESS=true
CRAWLER_USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36

# === 爬取限制 ===
# LLM 解析的最大 HTML 长度（超过会截断）
CRAWLER_MAX_HTML_LENGTH=50000
# 默认请求间隔（秒），防止被封禁
CRAWLER_DEFAULT_DELAY=1.0
# 默认最大爬取深度
CRAWLER_DEFAULT_MAX_DEPTH=3
# 默认最大页面数
CRAWLER_DEFAULT_MAX_PAGES=500

# === 调度配置 ===
# 定时检查站点调度配置的间隔（分钟）
CRAWLER_SCHEDULE_CHECK_INTERVAL=5